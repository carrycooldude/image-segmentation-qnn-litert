# LiteRT Image Segmentation with QNN

Real-time image segmentation Android app powered by LiteRT (Google's new TensorFlow Lite runtime) with Qualcomm Neural Network (QNN) acceleration.

## Features

- **Automatic NPUâ†’GPUâ†’CPU fallback** âš¡ - Intelligently selects best accelerator
- **Real-time segmentation** using camera or gallery images
- **21-class segmentation**: person, car, cat, dog, bicycle, and more
- **Visual backend indicators** - See which accelerator is active (NPU âš¡, GPU ğŸ”¥, CPU âš™ï¸)
- **QNN acceleration** for optimized inference on Qualcomm devices
- **JIT compilation**: On-device model compilation for optimal performance

## Performance

**Verified on Samsung S25 Ultra (Snapdragon 8 Elite):**

| Backend | Status | Inference Time | Auto-Fallback |
|---------|--------|----------------|---------------|
| **NPU** | âœ… Working | **~85ms** âš¡ | **1st Priority** |
| GPU     | âš ï¸ Variable | ~40-50ms | 2nd Priority |
| CPU     | âœ… Working | ~130ms | Final Fallback |

> **Note:** The app automatically tries NPU first, then GPU, then CPU. No manual selection needed!

## Supported Devices

Qualcomm Snapdragon devices with NPU support:
- SM8450 (Snapdragon 8 Gen 1)
- SM8550 (Snapdragon 8 Gen 2)
- SM8650 (Snapdragon 8 Gen 3)
- SM8750 (Snapdragon 8 Elite)
- SM8850 (Future)

## Quick Start

### Prerequisites

- Android Studio
- Android device with Qualcomm SoC and NPU support
- Android API level 31+

### Build and Run

1. Clone the repository:
```bash
git clone https://github.com/carrycooldude/image-segmentation-qnn-litert.git
cd image-segmentation-qnn-litert
```

2. Download QNN runtime libraries (automatically handled by build script):
```bash
bash fetch_qualcomm_library.sh
```

3. Build and install:
```bash
./gradlew installDebug
```

## How It Works

### Automatic Backend Selection

The app implements intelligent fallback logic at startup:

1. **Attempts NPU initialization** - Best performance on Snapdragon devices
2. **If NPU fails â†’ tries GPU** - Good performance, broader compatibility  
3. **If GPU fails â†’ uses CPU** - Guaranteed to work on all devices

```kotlin
// Automatic fallback on app launch
initSegmenterWithFallback()  // NPU â†’ GPU â†’ CPU
```

### Visual Indicators

The bottom sheet shows which backend is currently active:
- **NPU âš¡** - Fastest, using Qualcomm AI Engine
- **GPU ğŸ”¥** - Fast, using GPU acceleration
- **CPU âš™ï¸** - Compatible, CPU-only inference

You can also manually override the backend selection from the dropdown menu.

## Technical Details

- **Model**: 256x256x3 RGB input â†’ 6-channel output (21 classes)
- **Runtime**: LiteRT 2.0.0-alpha
- **Acceleration**: Qualcomm AI Engine Direct via QNN
- **Mode**: JIT (Just-In-Time) compilation
- **Architecture**: Dynamic feature modules for on-demand runtime loading

## QNN Runtime Versions

The app includes Qualcomm runtime libraries for multiple QNN versions:
- v69 (SM8450)
- v73 (SM8550)
- v75 (SM8650)
- v79 (SM8750)
- v81 (SM8850)

Device SoC is automatically detected and the appropriate runtime is loaded.

## Project Structure

```
android_jit/
â”œâ”€â”€ app/                           # Main application
â”‚   â”œâ”€â”€ src/main/assets/          # Model file
â”‚   â””â”€â”€ src/main/java/            # Kotlin source code
â”œâ”€â”€ litert_npu_runtime_libraries/ # QNN runtime modules
â”‚   â”œâ”€â”€ qualcomm_runtime_v69/
â”‚   â”œâ”€â”€ qualcomm_runtime_v73/
â”‚   â”œâ”€â”€ qualcomm_runtime_v75/
â”‚   â”œâ”€â”€ qualcomm_runtime_v79/
â”‚   â””â”€â”€ qualcomm_runtime_v81/
â””â”€â”€ fetch_qualcomm_library.sh     # Download Qualcomm SDK libs
```


## References

- [LiteRT Documentation](https://ai.google.dev/edge/litert)
- [LiteRT NPU Guide](https://ai.google.dev/edge/litert/next/npu)
- [Qualcomm AI Engine Direct](https://ai.google.dev/edge/litert/next/qualcomm)
